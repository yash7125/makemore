{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0461f89e",
      "metadata": {
        "id": "0461f89e"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a07b28d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a07b28d",
        "outputId": "48f3a7fd-7811-48f4-ae29-8296ed6985cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from typing import Any, Dict, Iterable, Optional\n",
        "import math\n",
        "import os\n",
        "import regex as re\n",
        "import tiktoken\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312218a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "312218a0",
        "outputId": "9df4ed5a-205d-4e4f-b3e3-7463bdd283bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of tokens: ['hello', ' world', '!', ' how', ' are', ' you', '?', ' I', \"'m\", ' fine', ',', ' thank', ' you', '!😊']\n"
          ]
        }
      ],
      "source": [
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "list_of_tokens = re.findall(gpt2pat, \"hello world! how are you? I'm fine, thank you!😊\")\n",
        "print(\"List of tokens:\", list_of_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173c2439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173c2439",
        "outputId": "91e15725-1469-479d-c677-d037b7c1a7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded tokens: [31373, 995, 0, 703, 389, 345, 30, 314, 1101, 3734, 11, 5875, 345, 0, 47249, 232]\n",
            "Encoded tokens: [15339, 1917, 0, 1268, 527, 499, 30, 358, 2846, 7060, 11, 9901, 499, 0, 76460, 232]\n",
            "Encoded tokens from example: [5269, 527, 499, 30]\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# Get the GPT-2 encoding --> GPT-2 does not merge spaces, so it's bad for coding\n",
        "enc_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "enc_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "list_of_tokens = enc_gpt2.encode(\"hello world! how are you? I'm fine, thank you!😊\")\n",
        "print(\"Encoded tokens:\", list_of_tokens)\n",
        "\n",
        "list_of_tokens = enc_gpt4.encode(\"hello world! how are you? I'm fine, thank you!😊\")\n",
        "print(\"Encoded tokens:\", list_of_tokens)\n",
        "\n",
        "# gpt-4 merges spaces, so it's better for coding\n",
        "list_of_tokens = enc_gpt4.encode(\"how are you?\")\n",
        "print(\"Encoded tokens from example:\", list_of_tokens)\n",
        "print(len(list_of_tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e4c7c9",
      "metadata": {
        "id": "59e4c7c9"
      },
      "outputs": [],
      "source": [
        "# # special tokens\n",
        "# special_tokens = {'<|endoftext|>', 50256}\n",
        "# # minbpe - GPT-4\n",
        "# # SentencePiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb08999",
      "metadata": {
        "id": "5bb08999"
      },
      "outputs": [],
      "source": [
        "with open(r\"tinystories_sample_5M.txt\", 'r', encoding='utf-8') as file:\n",
        "    sample_text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca02f013",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca02f013",
        "outputId": "cca343bb-fdf0-493c-8702-da29e1859036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
            "<|endoftext|>\n",
            "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
            "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
            "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
            "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear t\n"
          ]
        }
      ],
      "source": [
        "# sample_text_data[1:1000]\n",
        "# text_split = sample_text_data.split(\"<|endoftext|>\")\n",
        "# len(text_split)\n",
        "# print(text_split)\n",
        "print(sample_text_data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb41176a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb41176a",
        "outputId": "120ddc82-3706-47c6-9d9f-ca1fe3a08364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6458\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5158939"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# train a BPE\n",
        "# pre-tokenising sample_text_data\n",
        "# before pre-tokenising we will split the text around all the special tokens\n",
        "text_split = sample_text_data.split(\"<|endoftext|>\")\n",
        "print(len(text_split))\n",
        "\n",
        "gpt2pre_tok = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for i in range(len(text_split)):\n",
        "\n",
        "    text_data = text_split[i]\n",
        "    pre_tokens = re.findall(gpt2pre_tok, text_data)\n",
        "\n",
        "    for l in range(len(pre_tokens)):\n",
        "        pre_tokens[l] = pre_tokens[l].encode('utf-8')\n",
        "\n",
        "# # for i in range(len(pre_tok)):\n",
        "# #     print(list(pre_tok[i]))\n",
        "\n",
        "    for l in range(len(pre_tokens)):\n",
        "        for j in range(len(pre_tokens[l])):\n",
        "            tokens.append(pre_tokens[l][j])\n",
        "\n",
        "    # print(len(tokens))\n",
        "\n",
        "# text_to_bytes = sample_text_data.encode('utf-8')\n",
        "# list(text_to_bytes)\n",
        "# print(len(text_to_bytes))\n",
        "# print(len(sample_text_data))\n",
        "\n",
        "# len(text_to_bytes)\n",
        "# list(text_to_bytes)\n",
        "\n",
        "len(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc9c037",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adc9c037",
        "outputId": "be28c2aa-9b55-4c28-dff2-dfea8f7f8d52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# list(tokens)\n",
        "max_token = -1\n",
        "list_of_tokens = list(map(int, tokens))\n",
        "\n",
        "for i in range(len(list_of_tokens)):\n",
        "    if list_of_tokens[i] > max_token:\n",
        "        max_token = list_of_tokens[i]\n",
        "\n",
        "max_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6987612",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6987612",
        "outputId": "9667f141-c4e2-469e-cf75-3e91d3246d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 1389977\n"
          ]
        }
      ],
      "source": [
        "# BPE implementation\n",
        "\n",
        "def get_frequency_of_pairs(tokens):\n",
        "\n",
        "    counts = {}\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        pair = (tokens[i], tokens[i + 1])\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "def get_max_frequent_pair(counts):\n",
        "\n",
        "    sorted_pairs = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    ch1, ch2 = sorted_pairs[0][0]\n",
        "\n",
        "    if ch2 == None:\n",
        "        for i in range(len(counts)):\n",
        "            if sorted_pairs[i][0][1] != None:\n",
        "                return sorted_pairs[i][0], sorted_pairs[i][1]\n",
        "    else:\n",
        "        get_pair = sorted_pairs[0][0]\n",
        "        get_the_frequency = sorted_pairs[0][1]\n",
        "\n",
        "        return get_pair, get_the_frequency\n",
        "\n",
        "\n",
        "counts = get_frequency_of_pairs(list_of_tokens)\n",
        "get_pair, cnts = get_max_frequent_pair(counts)\n",
        "x1, x2 = get_pair\n",
        "\n",
        "def merge_tokens(ids, pair, idx):\n",
        "\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            new_tokens.append(idx)  # Merging the pair into a single token\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(ids[i])\n",
        "            i += 1\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "merges = {}\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "    counts = get_frequency_of_pairs(list_of_tokens)\n",
        "    get_pair, cnts = get_max_frequent_pair(counts)\n",
        "\n",
        "    if cnts == 1:\n",
        "        print(f\"iter = {i}\")\n",
        "        break\n",
        "    else:\n",
        "        # print(f\"Iteration {i + 1}: Pair {get_pair} with frequency {cnts}\")\n",
        "        list_of_tokens = merge_tokens(list_of_tokens, get_pair, i + 256)  # Using 256 as the starting index for new tokens\n",
        "        merges[get_pair] = i + 256\n",
        "        # print(f\"Iteration {i + 1}: Pair {get_pair} with merged token = {i + 256}\")\n",
        "\n",
        "\n",
        "print(\"Number of tokens:\", len(list_of_tokens))\n",
        "# print(\"Final tokens:\", tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbdbdf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbdbdf7",
        "outputId": "92eac122-2130-4d17-8fc2-0c87f325a48d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.711528320252781"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "compression_ratio = len(tokens) / len(list_of_tokens)\n",
        "compression_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e360f58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e360f58",
        "outputId": "dd187788-3fff-4d82-e7c9-67cb8f530790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(101, 32): 256, (100, 32): 257, (116, 104): 258, (32, 97): 259, (46, 32): 260, (116, 32): 261, (121, 32): 262, (115, 32): 263, (110, 257): 264, (116, 111): 265, (101, 114): 266, (101, 257): 267, (258, 256): 268, (44, 32): 269, (119, 97): 270, (105, 110): 271, (104, 256): 272, (265, 32): 273, (111, 117): 274, (259, 264): 275, (101, 110): 276, (104, 97): 277, (260, 84): 278, (259, 32): 279, (111, 109): 280, (115, 97): 281, (97, 114): 282, (32, 268): 283, (111, 110): 284, (104, 101): 285, (46, 10): 286, (105, 109): 287, (108, 108): 288, (103, 32): 289, (270, 263): 290, (97, 110): 291, (111, 114): 292, (266, 32): 293, (105, 116): 294, (97, 121): 295, (105, 100): 296, (105, 114): 297, (105, 263): 298, (114, 101): 299, (112, 108): 300, (105, 108): 301, (267, 273): 302, (119, 105): 303, (97, 109): 304, (258, 101): 305, (108, 111): 306, (115, 116): 307, (114, 105): 308, (97, 32): 309, (97, 264): 310, (260, 72): 311, (260, 83): 312, (111, 32): 313, (285, 262): 314, (311, 256): 315, (32, 104): 316, (110, 111): 317, (97, 261): 318, (266, 256): 319, (79, 110): 320, (303, 258): 321, (117, 110): 322, (278, 314): 323, (271, 289): 324, (112, 112): 325, (118, 266): 326, (278, 272): 327, (99, 107): 328, (108, 105): 329, (34, 32): 330, (98, 105): 331, (288, 32): 332, (108, 101): 333, (312, 272): 334, (269, 34): 335, (100, 295): 336, (117, 261): 337, (280, 32): 338, (98, 101): 339, (105, 261): 340, (119, 32): 341, (111, 107): 342, (281, 296): 343, (121, 274): 344, (271, 103): 345, (108, 256): 346, (98, 111): 347, (109, 97): 348, (277, 325): 349, (101, 100): 350, (100, 111): 351, (108, 257): 352, (98, 97): 353, (117, 112): 354, (276, 100): 355, (287, 32): 356, (111, 102): 357, (103, 104): 358, (102, 308): 359, (110, 101): 360, (97, 262): 361, (115, 104): 362, (326, 262): 363, (291, 32): 364, (99, 104): 365, (115, 101): 366, (102, 292): 367, (108, 32): 368, (270, 110): 369, (276, 32): 370, (99, 256): 371, (270, 115): 372, (277, 257): 373, (294, 116): 374, (331, 289): 375, (343, 335): 376, (359, 355): 377, (76, 301): 378, (101, 269): 379, (118, 256): 380, (114, 111): 381, (286, 10): 382, (305, 262): 383, (116, 302): 384, (317, 261): 385, (300, 295): 386, (320, 256): 387, (260, 73): 388, (103, 101): 389, (374, 346): 390, (108, 390): 391, (116, 287): 392, (104, 298): 393, (372, 279): 394, (107, 256): 395, (39, 263): 396, (258, 318): 397, (101, 115): 398, (97, 116): 399, (110, 304): 400, (387, 336): 401, (115, 272): 402, (274, 352): 403, (271, 283): 404, (105, 257): 405, (99, 282): 406, (400, 267): 407, (104, 293): 408, (98, 297): 409, (121, 275): 410, (84, 272): 411, (258, 319): 412, (258, 266): 413, (273, 268): 414, (114, 97): 415, (267, 310): 416, (119, 319): 417, (102, 101): 418, (284, 279): 419, (276, 261): 420, (115, 313): 421, (119, 104): 422, (281, 119): 423, (34, 10): 424, (300, 361): 425, (103, 111): 426, (44, 275): 427, (285, 108): 428, (369, 384): 429, (344, 32): 430, (114, 256): 431, (98, 337): 432, (320, 371): 433, (428, 112): 434, (306, 342): 435, (119, 420): 436, (115, 256): 437, (73, 32): 438, (354, 419): 439, (433, 439): 440, (440, 392): 441, (267, 268): 442, (316, 298): 443, (121, 259): 444, (274, 264): 445, (321, 32): 446, (33, 32): 447, (412, 394): 448, (102, 105): 449, (116, 97): 450, (115, 107): 451, (112, 111): 452, (102, 322): 453, (108, 97): 454, (99, 97): 455, (84, 356): 456, (388, 261): 457, (441, 379): 458, (101, 97): 459, (401, 269): 460, (117, 114): 461, (265, 389): 462, (115, 275): 463, (269, 432): 464, (409, 257): 465, (378, 262): 466, (115, 280): 467, (98, 256): 468, (316, 293): 469, (357, 32): 470, (84, 287): 471, (33, 330): 472, (116, 299): 473, (109, 262): 474, (107, 32): 475, (115, 261): 476, (109, 32): 477, (328, 32): 478, (112, 282): 479, (290, 363): 480, (103, 297): 481, (304, 256): 482, (297, 32): 483, (44, 279): 484, (101, 116): 485, (115, 109): 486, (119, 110): 487, (109, 338): 488, (101, 99): 489, (382, 458): 490, (99, 482): 491, (109, 280): 492, (99, 111): 493, (423, 279): 494, (360, 341): 495, (103, 97): 496, (102, 117): 497, (260, 66): 498, (261, 268): 499, (116, 116): 500, (305, 483): 501, (306, 118): 502, (256, 310): 503, (284, 283): 504, (105, 99): 505, (121, 269): 506, (77, 97): 507, (365, 32): 508, (377, 115): 509, (367, 32): 510, (462, 413): 511, (507, 120): 512, (100, 405): 513, (39, 261): 514, (46, 330): 515, (481, 368): 516, (105, 115): 517, (386, 267): 518, (274, 358): 519, (277, 380): 520, (117, 109): 521, (373, 309): 522, (486, 301): 523, (260, 65): 524, (261, 273): 525, (99, 403): 526, (102, 97): 527, (46, 424): 528, (286, 34): 529, (349, 121): 530, (99, 364): 531, (278, 356): 532, (101, 263): 533, (115, 296): 534, (117, 256): 535, (101, 120): 536, (98, 117): 537, (102, 445): 538, (109, 292): 539, (435, 267): 540, (329, 107): 541, (270, 121): 542, (307, 282): 543, (111, 119): 544, (84, 280): 545, (115, 265): 546, (349, 262): 547, (490, 448): 548, (513, 385): 549, (347, 262): 550, (271, 32): 551, (110, 100): 552, (102, 114): 553, (84, 338): 554, (418, 108): 555, (281, 405): 556, (97, 108): 557, (265, 121): 558, (351, 289): 559, (284, 32): 560, (119, 403): 561, (277, 261): 562, (467, 101): 563, (274, 116): 564, (100, 361): 565, (89, 274): 566, (115, 112): 567, (353, 288): 568, (258, 293): 569, (299, 97): 570, (111, 257): 571, (101, 108): 572, (119, 292): 573, (259, 110): 574, (107, 101): 575, (104, 111): 576, (329, 395): 577, (260, 34): 578, (116, 266): 579, (108, 107): 580, (99, 318): 581, (284, 256): 582, (321, 283): 583, (258, 324): 584, (112, 32): 585, (63, 330): 586, (555, 261): 587, (111, 269): 588, (114, 32): 589, (118, 267): 590, (112, 105): 591, (276, 116): 592, (83, 304): 593, (103, 114): 594, (286, 411): 595, (331, 103): 596, (348, 395): 597, (115, 406): 598, (115, 348): 599, (109, 117): 600, (109, 101): 601, (100, 100): 602, (290, 421): 603, (100, 256): 604, (258, 345): 605, (502, 302): 606, (115, 277): 607, (258, 519): 608, (87, 104): 609, (391, 516): 610, (401, 484): 611, (271, 279): 612, (260, 460): 613, (357, 283): 614, (103, 313): 615, (105, 358): 616, (479, 107): 617, (498, 337): 618, (382, 10): 619, (543, 384): 620, (117, 103): 621, (541, 302): 622, (274, 261): 623, (366, 256): 624, (109, 291): 625, (269, 268): 626, (348, 604): 627, (117, 358): 628, (347, 120): 629, (512, 32): 630, (102, 111): 631, (449, 264): 632, (116, 308): 633, (282, 110): 634, (101, 288): 635, (112, 97): 636, (99, 280): 637, (77, 105): 638, (321, 443): 639, (351, 487): 640, (340, 290): 641, (265, 111): 642, (377, 263): 643, (369, 525): 644, (107, 317): 645, (265, 342): 646, (286, 460): 647, (84, 314): 648, (453, 32): 649, (116, 114): 650, (259, 431): 651, (305, 109): 652, (563, 584): 653, (99, 399): 654, (496, 271): 655, (281, 121): 656, (111, 98): 657, (434, 32): 658, (112, 337): 659, (111, 112): 660, (286, 65): 661, (115, 322): 662, (117, 99): 663, (333, 634): 664, (112, 117): 665, (108, 262): 666, (102, 32): 667, (329, 590): 668, (108, 284): 669, (281, 100): 670, (270, 580): 671, (103, 299): 672, (112, 299): 673, (538, 309): 674, (97, 288): 675, (117, 437): 676, (265, 262): 677, (281, 341): 678, (113, 117): 679, (303, 332): 680, (100, 105): 681, (550, 407): 682, (101, 326): 683, (275, 268): 684, (378, 121): 685, (260, 10): 686, (285, 444): 687, (116, 267): 688, (353, 478): 689, (409, 100): 690, (111, 100): 691, (114, 267): 692, (112, 114): 693, (426, 571): 694, (110, 514): 695, (497, 368): 696, (118, 293): 697, (103, 105): 698, (98, 114): 699, (258, 32): 700, (115, 119): 701, (349, 410): 702, (99, 101): 703, (76, 663): 704, (108, 100): 705, (104, 266): 706, (344, 589): 707, (473, 101): 708, (467, 256): 709, (111, 413): 710, (104, 287): 711, (83, 452): 712, (77, 280): 713, (110, 97): 714, (306, 119): 715, (104, 280): 716, (454, 628): 717, (44, 330): 718, (278, 687): 719, (112, 101): 720, (523, 416): 721, (111, 341): 722, (97, 368): 723, (116, 263): 724, (382, 611): 725, (257, 310): 726, (115, 115): 727, (100, 266): 728, (276, 269): 729, (119, 562): 730, (422, 370): 731, (305, 477): 732, (277, 110): 733, (267, 408): 734, (105, 371): 735, (274, 552): 736, (119, 256): 737, (386, 324): 738, (351, 103): 739, (66, 370): 740, (33, 424): 741, (566, 32): 742, (307, 105): 743, (97, 451): 744, (100, 97): 745, (282, 32): 746, (388, 32): 747, (353, 328): 748, (100, 489): 749, (599, 332): 750, (422, 313): 751, (83, 117): 752, (259, 332): 753, (265, 313): 754, (398, 269): 755, (353, 332): 756, (99, 294): 757, (610, 407): 758, (494, 375): 759, (117, 116): 760, (107, 495): 761, (270, 116): 762, (109, 111): 763, (83, 535): 764, (110, 262): 765, (116, 396): 766, (226, 128): 767, (536, 757): 768, (608, 261): 769, (425, 446): 770, (542, 263): 771, (625, 262): 772, (99, 306): 773, (749, 296): 774, (321, 469): 775, (259, 114): 776, (279, 375): 777, (99, 274): 778, (436, 414): 779, (339, 491): 780, (455, 676): 781, (564, 534): 782, (596, 269): 783, (87, 256): 784, (521, 112): 785, (115, 292): 786, (362, 32): 787, (114, 262): 788, (323, 417): 789, (102, 102): 790, (267, 393): 791, (99, 108): 792, (104, 274): 793, (339, 781): 794, (381, 280): 795, (275, 376): 796, (106, 117): 797, (347, 700): 798, (98, 333): 799, (656, 115): 800, (256, 268): 801, (114, 291): 802, (114, 364): 803, (278, 338): 804, (114, 121): 805, (553, 280): 806, (339, 476): 807, (285, 282): 808, (426, 261): 809, (101, 264): 810, (333, 97): 811, (284, 269): 812, (633, 302): 813, (105, 288): 814, (284, 514): 815, (282, 100): 816, (100, 313): 817, (284, 101): 818, (114, 322): 819, (115, 461): 820, (112, 308): 821, (336, 269): 822, (267, 397): 823, (66, 337): 824, (378, 410): 825, (265, 352): 826, (459, 508): 827, (406, 101): 828, (638, 309): 829, (673, 500): 830, (66, 276): 831, (256, 273): 832, (774, 302): 833, (259, 98): 834, (423, 283): 835, (271, 534): 836, (115, 111): 837, (260, 70): 838, (496, 380): 839, (105, 264): 840, (367, 279): 841, (76, 485): 842, (300, 97): 843, (327, 465): 844, (259, 451): 845, (274, 257): 846, (593, 32): 847, (109, 364): 848, (260, 466): 849, (820, 821): 850, (354, 32): 851, (108, 117): 852, (660, 276): 853, (116, 275): 854, (524, 264): 855, (98, 262): 856, (539, 256): 857, (277, 114): 858, (415, 98): 859, (115, 117): 860, (842, 396): 861, (99, 291): 862, (102, 715): 863, (65, 110): 864, (497, 108): 865, (104, 621): 866, (99, 333): 867, (118, 101): 868, (100, 296): 869, (258, 291): 870, (89, 755): 871, (547, 273): 872, (106, 785): 873, (290, 385): 874, (101, 112): 875, (259, 542): 876, (99, 299): 877, (108, 121): 878, (116, 461): 879, (362, 544): 880, (609, 370): 881, (450, 395): 882, (540, 318): 883, (101, 363): 884, (362, 271): 885, (338, 397): 886, (115, 269): 887, (317, 116): 888, (110, 32): 889, (260, 69): 890, (349, 276): 891, (500, 266): 892, (259, 108): 893, (259, 655): 894, (307, 114): 895, (66, 657): 896, (391, 682): 897, (67, 364): 898, (645, 341): 899, (733, 475): 900, (539, 101): 901, (704, 262): 902, (115, 105): 903, (259, 288): 904, (83, 282): 905, (258, 298): 906, (99, 285): 907, (106, 111): 908, (294, 256): 909, (102, 333): 910, (294, 263): 911, (111, 569): 912, (886, 565): 913, (567, 489): 914, (270, 262): 915, (683, 121): 916, (101, 118): 917, (462, 569): 918, (305, 444): 919, (63, 424): 920, (389, 261): 921, (109, 121): 922, (329, 307): 923, (527, 307): 924, (339, 97): 925, (104, 356): 926, (457, 290): 927, (383, 417): 928, (345, 275): 929, (367, 283): 930, (270, 114): 931, (100, 262): 932, (284, 103): 933, (598, 350): 934, (301, 262): 935, (325, 267): 936, (270, 579): 937, (321, 279): 938, (436, 273): 939, (322, 728): 940, (287, 557): 941, (464, 272): 942, (119, 101): 943, (376, 438): 944, (100, 291): 945, (115, 323): 946, (111, 352): 947, (526, 385): 948, (100, 101): 949, (573, 107): 950, (344, 269): 951, (116, 117): 952, (869, 695): 953, (914, 105): 954, (84, 900): 955, (345, 283): 956, (112, 266): 957, (278, 104): 958, (473, 256): 959, (328, 267): 960, (111, 300): 961, (102, 579): 962, (605, 263): 963, (109, 256): 964, (358, 116): 965, (72, 256): 966, (296, 459): 967, (827, 710): 968, (260, 742): 969, (808, 257): 970, (315, 429): 971, (103, 256): 972, (316, 287): 973, (396, 488): 974, (724, 470): 975, (884, 336): 976, (259, 499): 977, (398, 275): 978, (891, 350): 979, (269, 272): 980, (360, 282): 981, (669, 103): 982, (381, 328): 983, (260, 784): 984, (870, 107): 985, (292, 32): 986, (105, 667): 987, (100, 299): 988, (306, 975): 989, (863, 266): 990, (602, 276): 991, (359, 810): 992, (776, 445): 993, (110, 735): 994, (471, 275): 995, (793, 366): 996, (288, 267): 997, (479, 475): 998, (301, 256): 999, (271, 414): 1000, (350, 335): 1001, (98, 346): 1002, (599, 288): 1003, (536, 112): 1004, (360, 697): 1005, (277, 552): 1006, (102, 116): 1007, (98, 108): 1008, (117, 115): 1009, (806, 283): 1010, (97, 332): 1011, (323, 518): 1012, (109, 274): 1013, (558, 115): 1014, (913, 812): 1015, (797, 476): 1016, (285, 97): 1017, (698, 380): 1018, (101, 307): 1019, (281, 726): 1020, (316, 266): 1021, (107, 840): 1022, (762, 365): 1023, (712, 261): 1024, (101, 318): 1025, (879, 110): 1026, (404, 617): 1027, (121, 901): 1028, (118, 324): 1029, (830, 262): 1030, (720, 961): 1031, (717, 416): 1032, (101, 261): 1033, (102, 304): 1034, (493, 108): 1035, (117, 328): 1036, (74, 111): 1037, (427, 268): 1038, (278, 287): 1039, (566, 651): 1040, (721, 376): 1041, (277, 116): 1042, (32, 290): 1043, (850, 115): 1044, (104, 461): 1045, (545, 275): 1046, (77, 338): 1047, (102, 271): 1048, (75, 374): 1049, (490, 612): 1050, (102, 297): 1051, (100, 815): 1052, (116, 119): 1053, (792, 287): 1054, (301, 32): 1055, (102, 381): 1056, (83, 272): 1057, (347, 342): 1058, (312, 535): 1059, (101, 368): 1060, (719, 332): 1061, (354, 275): 1062, (281, 257): 1063, (619, 10): 1064, (646, 32): 1065, (505, 101): 1066, (1004, 489): 1067, (716, 101): 1068, (98, 266): 1069, (99, 117): 1070, (273, 468): 1071, (258, 370): 1072, (39, 477): 1073, (327, 581): 1074, (664, 823): 1075, (263, 268): 1076, (103, 816): 1077, (115, 679): 1078, (638, 97): 1079, (1054, 98): 1080, (322, 116): 1081, (33, 10): 1082, (675, 262): 1083, (291, 100): 1084, (103, 103): 1085, (627, 309): 1086, (104, 616): 1087, (260, 630): 1088, (273, 425): 1089, (68, 97): 1090, (103, 117): 1091, (487, 32): 1092, (116, 105): 1093, (606, 425): 1094, (452, 261): 1095, (520, 309): 1096, (778, 705): 1097, (102, 282): 1098, (311, 298): 1099, (917, 370): 1100, (312, 588): 1101, (115, 315): 1102, (322, 1067): 1103, (286, 648): 1104, (101, 585): 1105, (108, 296): 1106, (366, 108): 1107, (360, 120): 1108, (838, 114): 1109, (273, 393): 1110, (269, 383): 1111, (457, 394): 1112, (73, 259): 1113, (271, 107): 1114, (111, 342): 1115, (752, 101): 1116, (317, 119): 1117, (422, 319): 1118, (559, 407): 1119, (74, 97): 1120, (112, 505): 1121, (115, 334): 1122, (269, 421): 1123, (607, 431): 1124, (1103, 688): 1125, (553, 338): 1126, (954, 723): 1127, (591, 960): 1128, (259, 261): 1129, (267, 309): 1130, (288, 262): 1131, (471, 532): 1132, (745, 257): 1133, (905, 97): 1134, (600, 508): 1135, (274, 499): 1136, (101, 282): 1137, (348, 107): 1138, (116, 415): 1139, (893, 771): 1140, (114, 506): 1141, (315, 290): 1142, (304, 32): 1143, (637, 256): 1144, (294, 275): 1145, (427, 383): 1146, (115, 1106): 1147, (362, 403): 1148, (594, 736): 1149, (272, 290): 1150, (115, 357): 1151, (122, 122): 1152, (116, 279): 1153, (955, 951): 1154, (98, 121): 1155, (373, 364): 1156, (426, 691): 1157, (1035, 292): 1158, (616, 116): 1159, (450, 580): 1160, (271, 475): 1161, (923, 276): 1162, (286, 456): 1163, (286, 824): 1164, (259, 263): 1165, (653, 1125): 1166, (925, 760): 1167, (115, 279): 1168, (258, 399): 1169, (108, 506): 1170, (282, 445): 1171, (669, 289): 1172, (671, 267): 1173, (1166, 979): 1174, (852, 790): 1175, (266, 275): 1176, (493, 342): 1177, (418, 572): 1178, (117, 478): 1179, (418, 1060): 1180, (98, 415): 1181, (646, 283): 1182, (271, 443): 1183, (629, 32): 1184, (645, 119): 1185, (505, 32): 1186, (1167, 105): 1187, (866, 103): 1188, (659, 268): 1189, (607, 114): 1190, (1121, 952): 1191, (110, 309): 1192, (492, 275): 1193, (458, 448): 1194, (480, 530): 1195, (282, 107): 1196, (286, 84): 1197, (111, 108): 1198, (388, 110): 1199, (351, 292): 1200, (449, 787): 1201, (107, 296): 1202, (334, 290): 1203, (354, 283): 1204, (306, 328): 1205, (558, 263): 1206, (834, 623): 1207, (699, 342): 1208, (267, 340): 1209, (317, 105): 1210, (523, 350): 1211, (306, 380): 1212, (348, 103): 1213, (327, 559): 1214, (121, 521): 1215, (515, 411): 1216, (271, 469): 1217, (712, 116): 1218, (622, 425): 1219, (324, 273): 1220, (265, 279): 1221, (1108, 261): 1222, (481, 108): 1223, (377, 269): 1224, (406, 32): 1225, (672, 318): 1226, (836, 101): 1227, (100, 415): 1228, (435, 32): 1229, (743, 332): 1230, (668, 612): 1231, (1007, 293): 1232, (451, 121): 1233, (605, 115): 1234, (693, 846): 1235, (261, 470): 1236, (316, 280): 1237, (119, 271): 1238, (274, 263): 1239, (73, 1073): 1240, (392, 101): 1241, (115, 263): 1242, (864, 714): 1243, (753, 336): 1244, (260, 881): 1245, (109, 485): 1246, (360, 119): 1247, (544, 269): 1248, (99, 277): 1249, (258, 729): 1250, (101, 109): 1251, (111, 326): 1252, (397, 272): 1253, (376, 871): 1254, (349, 935): 1255}\n"
          ]
        }
      ],
      "source": [
        "# list(tokens)\n",
        "max_token = -1\n",
        "# list_of_tokens = list(map(int, tokens))\n",
        "\n",
        "for i in range(len(list_of_tokens)):\n",
        "    if list_of_tokens[i] > max_token:\n",
        "        max_token = list_of_tokens[i]\n",
        "\n",
        "# max_token\n",
        "# for <|endoftext|>\n",
        "# list_of_tokens.append(415)\n",
        "print(merges)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzCTsK4ccwof",
        "outputId": "48bc5bad-cd90-4987-b868-a570aba68a9a"
      },
      "id": "HzCTsK4ccwof",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2f9b9a",
      "metadata": {
        "id": "9d2f9b9a"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "\n",
        "    # pre-tokenising sample_text_data\n",
        "    # before pre-tokenising we will split the text around all the special tokens\n",
        "    text_split = text.split(\"<|endoftext|>\")\n",
        "    # print(f\"len of text split = {len(text_split)}\")\n",
        "\n",
        "    # gpt2pre_tok = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for i in range(len(text_split)):\n",
        "\n",
        "        text_data = text_split[i]\n",
        "        pre_tokens = re.findall(gpt2pre_tok, text_data)\n",
        "\n",
        "        for l in range(len(pre_tokens)):\n",
        "            pre_tokens[l] = pre_tokens[l].encode('utf-8')\n",
        "\n",
        "    # # for i in range(len(pre_tok)):\n",
        "    # #     print(list(pre_tok[i]))\n",
        "\n",
        "        for l in range(len(pre_tokens)):\n",
        "            for j in range(len(pre_tokens[l])):\n",
        "                tokens.append(pre_tokens[l][j])\n",
        "\n",
        "    while len(tokens) >= 2:\n",
        "\n",
        "        counts = get_frequency_of_pairs(tokens)\n",
        "        pair, cnts = get_max_frequent_pair(counts)\n",
        "\n",
        "        if pair not in merges:\n",
        "            break\n",
        "\n",
        "        tokens = merge_tokens(tokens, pair, merges[pair])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e05f65f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e05f65f",
        "outputId": "2b288992-8a91-4c79-9ea3-f0ee40db8d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size =  1256\n"
          ]
        }
      ],
      "source": [
        "vocab_size = max_token + 1\n",
        "print(\"Vocabulary size = \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ef293b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ef293b",
        "outputId": "52f6c23b-d323-49b2-f3ab-f7a6e4361268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', ' name', ' is', ' <|', 'endoftext', '|>', ' Yash', ' Kumar']\n"
          ]
        }
      ],
      "source": [
        "# encoder = encode(\"My name is <|endoftext|> Yash Kumar\")\n",
        "# len(encoder)\n",
        "# # list(bytes([255]))\n",
        "s = \"My name is <|endoftext|> Yash Kumar\"\n",
        "words = re.findall(gpt2pre_tok, s)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47be9669",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "47be9669",
        "outputId": "70332d8d-4f3c-487b-bc31-0e20f9e1c520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My name is  Yash Kumar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# decoding part\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "# vocab[2]\n",
        "for (p1, p2), idx in merges.items():\n",
        "    # print(f\"Pair {p1}, {p2} is encoded as token {idx}\")\n",
        "    vocab[idx] = vocab[p1] + vocab[p2]\n",
        "\n",
        "# print(\"Vocabulary size:\", len(vocab))\n",
        "\n",
        "def decode(ids):\n",
        "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "    text = tokens.decode('utf-8', errors='replace')\n",
        "    return text\n",
        "\n",
        "# print(\"Decoded text:\", decode(tokens[:10]))\n",
        "decode(encode(\"My name is <|endoftext|> Yash Kumar\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362b881c",
      "metadata": {
        "id": "362b881c"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "context_length = 64\n",
        "num_heads = 8\n",
        "d_model = 128 # embedding dimension\n",
        "n_layers = 6\n",
        "vocab_size = vocab_size # 356\n",
        "max_steps = 20000\n",
        "warmup_steps = 100\n",
        "total_cycle_steps = 800\n",
        "max_lr = 9e-4\n",
        "min_lr = 9e-5\n",
        "weight_decay = 0.1\n",
        "grad_clip = 1.0\n",
        "eps = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6709d8e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6709d8e1",
        "outputId": "0a10547d-d206-4b80-dff5-fe0c263a08ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size = 1256\n"
          ]
        }
      ],
      "source": [
        "print(f\"vocab size = {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d284a7ba",
      "metadata": {
        "id": "d284a7ba"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "\n",
        "    def __init__(self, features_in: int, features_out: int, bias = False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter((torch.randn((features_in, features_out)) / (features_in ** 0.5)).to(device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = x @ self.weight.transpose(-2, -1)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ceb0b6",
      "metadata": {
        "id": "f5ceb0b6"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_embedding, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(num_embedding, embedding_dim).to(device))\n",
        "\n",
        "    def forward(self, idx):\n",
        "        self.out = self.weights[idx]\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weights]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75319c2",
      "metadata": {
        "id": "e75319c2"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
        "\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model).to(device))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        in_dtype = x.dtype\n",
        "        x = x.to(torch.float32)\n",
        "        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
        "        normalized_rmsx = x / rms\n",
        "        result = normalized_rmsx * self.gamma\n",
        "        return result.to(in_dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefe1048",
      "metadata": {
        "id": "aefe1048"
      },
      "outputs": [],
      "source": [
        "class SwiGLU_FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "\n",
        "        d_fff = (8 / 3) * d_model\n",
        "        self.d_ff = int(d_fff)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.w1 = Linear(self.d_ff, self.d_model)\n",
        "        self.w3 = Linear(self.d_ff, self.d_model)\n",
        "        self.w2 = Linear(self.d_model, self.d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x --> [d_model] --> [1, d_model]\n",
        "        # w1.T -------------> [d_model, d_ff]\n",
        "        silu = self.w1(x) * torch.sigmoid(self.w1(x))\n",
        "        # print(f\"silu shape = {silu.shape}\")\n",
        "        intermidiate = silu * self.w3(x)\n",
        "        # print(f\"intermidiate shape = {intermidiate.shape}\")\n",
        "        # inter --> [4, 32, d_ff]\n",
        "        # w2.T ---> [d_ff, d_model]\n",
        "        result = self.w2(intermidiate)\n",
        "        # print(f\"result shape = {result.shape}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbe4fe4",
      "metadata": {
        "id": "5fbe4fe4"
      },
      "outputs": [],
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Rotary Positional Embedding (RoPE) as described in https://arxiv.org/abs/2104.09864.\n",
        "\n",
        "    This module pre-computes the cosine and sine frequency tables and applies the rotation\n",
        "    to the input tensor during the forward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, theta: float, d_k: int, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Constructs the RoPE module.\n",
        "\n",
        "        Args:\n",
        "            theta (float): The base period for the sinusoidal embeddings. A common value is 10000.0.\n",
        "            d_k (int): The dimension of the query and key vectors. Must be an even number.\n",
        "            max_seq_len (int): The maximum sequence length that this module will process.\n",
        "            device (torch.device | None, optional): The device to store the buffers on. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Validate that the dimension d_k is an even number, as RoPE works on pairs of dimensions.\n",
        "        if d_k % 2 != 0:\n",
        "            raise ValueError(f\"Dimension d_k ({d_k}) must be an even number for RoPE.\")\n",
        "\n",
        "        self.theta = theta\n",
        "        self.d_k = d_k\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # Pre-compute the frequency terms (theta_j in the paper)\n",
        "        # The shape of freqs will be (d_k / 2)\n",
        "        freqs = 1.0 / (self.theta ** (torch.arange(0, self.d_k, 2).float() / self.d_k))\n",
        "\n",
        "        # Pre-compute the positions 'm'\n",
        "        # The shape of m will be (max_seq_len)\n",
        "        m = torch.arange(self.max_seq_len)\n",
        "\n",
        "        # Create the full frequency map by taking the outer product of m and freqs\n",
        "        # This results in a tensor of shape (max_seq_len, d_k / 2)\n",
        "        # Each element (i, j) corresponds to the angle m_i * theta_j\n",
        "        freqs_cis = torch.outer(m, freqs)\n",
        "\n",
        "        # Compute the cosine and sine values for all positions and frequencies\n",
        "        cos_cached = torch.cos(freqs_cis)\n",
        "        sin_cached = torch.sin(freqs_cis)\n",
        "\n",
        "        # Register the computed values as buffers. Buffers are part of the module's state\n",
        "        # but are not considered parameters to be trained. They are moved to the correct\n",
        "        # device when the model is moved (e.g., model.to(device)).\n",
        "        self.register_buffer(\"cos_cached\", cos_cached.to(device))\n",
        "        self.register_buffer(\"sin_cached\", sin_cached.to(device))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies RoPE to an input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of queries or keys.\n",
        "                              Shape: (..., seq_len, d_k)\n",
        "            token_positions (torch.Tensor): A tensor specifying the absolute positions\n",
        "                                             of tokens in x along the sequence dimension.\n",
        "                                             Shape: (..., seq_len)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The tensor with rotary positional embeddings applied.\n",
        "                          Shape: (..., seq_len, d_k)\n",
        "        \"\"\"\n",
        "        # Retrieve the pre-computed cosine and sine values using the token positions.\n",
        "        # The indexing operation will fetch the correct (cos, sin) pair for each token's position.\n",
        "        # The resulting shape will be (..., seq_len, d_k / 2)\n",
        "        cos = self.cos_cached[token_positions]\n",
        "        sin = self.sin_cached[token_positions]\n",
        "\n",
        "        # Reshape x to split the last dimension into pairs for rotation.\n",
        "        # x_orig shape: (..., seq_len, d_k)\n",
        "        # x_reshaped shape: (..., seq_len, d_k / 2, 2)\n",
        "        x_reshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "\n",
        "        # Split into the real (x_r) and imaginary (x_i) parts, corresponding to\n",
        "        # even and odd dimensions.\n",
        "        x_r, x_i = x_reshaped.unbind(-1)\n",
        "\n",
        "        # The RoPE transformation is equivalent to complex number multiplication:\n",
        "        # (x_r + i*x_i) * (cos + i*sin) = (x_r*cos - x_i*sin) + i*(x_r*sin + x_i*cos)\n",
        "\n",
        "        # Apply the rotation to the even-indexed dimensions (real part)\n",
        "        # Shape: (..., seq_len, d_k / 2)\n",
        "        rotated_r = x_r * cos - x_i * sin\n",
        "\n",
        "        # Apply the rotation to the odd-indexed dimensions (imaginary part)\n",
        "        # Shape: (..., seq_len, d_k / 2)\n",
        "        rotated_i = x_r * sin + x_i * cos\n",
        "\n",
        "        # Stack the rotated parts back together\n",
        "        # Shape: (..., seq_len, d_k / 2, 2)\n",
        "        rotated_pairs = torch.stack((rotated_r, rotated_i), dim=-1)\n",
        "\n",
        "        # Reshape back to the original input shape\n",
        "        # Shape: (..., seq_len, d_k)\n",
        "        rotated_x = rotated_pairs.flatten(start_dim=-2)\n",
        "\n",
        "        return rotated_x.type_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c76517",
      "metadata": {
        "id": "64c76517"
      },
      "outputs": [],
      "source": [
        "# m = nn.Softmax(dim=1)\n",
        "# input = torch.randn(2, 3)\n",
        "# print(input)\n",
        "# output = m(input)\n",
        "# print(f\"output = {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c71d1d",
      "metadata": {
        "id": "11c71d1d"
      },
      "outputs": [],
      "source": [
        "def Softmax(dim: int, input: torch.Tensor):\n",
        "\n",
        "    max_values, _ = torch.max(input, dim=dim, keepdim=True)\n",
        "    # print(f\"max value = {max_values}\")\n",
        "    final_inp = input - max_values\n",
        "    # print(f\"final input = {final_inp}\")\n",
        "    sum_val = torch.sum(torch.exp(final_inp), dim=dim, keepdim=True)\n",
        "    # print(f\"sum value = {sum_val}\")\n",
        "    result = torch.exp(final_inp) / sum_val\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d184373",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d184373",
        "outputId": "59dc8e88-31f7-48d7-bd1e-d10e5e92da94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8054e-01, 2.9539e-04, 1.1917e-01],\n",
              "        [4.5286e-05, 9.9748e-01, 2.4725e-03],\n",
              "        [1.6660e-05, 2.4726e-03, 9.9751e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "x = torch.tensor([[10, 2, 8],\n",
        "                  [5, 15, 9],\n",
        "                  [1, 6, 12]], dtype=torch.float32)\n",
        "\n",
        "y = Softmax(-1, x)\n",
        "y\n",
        "\n",
        "# m = Softmax(-2, x)\n",
        "# m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032b1444",
      "metadata": {
        "id": "032b1444"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, head_size: int):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.head_size = head_size\n",
        "\n",
        "        self.W_q = Linear(head_size, d_model)\n",
        "        self.W_k = Linear(head_size, d_model)\n",
        "        self.W_v = Linear(head_size, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        _, seq_len, _ = x.shape\n",
        "        #W -->            (d_model, head_size)\n",
        "        #x --> (batch_size, seq_len, d_model)\n",
        "        # print(f\"x shape = {x.shape}\")\n",
        "        # print(f\"W_q shape = {self.W_q.weight.shape}\")\n",
        "        q = self.W_q(x) # (batch_size, seq_len, d_k)\n",
        "        # print(f\"q shape = {q.shape}\")\n",
        "        k = self.W_k(x)\n",
        "        # print(f\"k shape = {k.shape}\")\n",
        "        v = self.W_v(x)\n",
        "        # print(f\"v shape = {v.shape}\")\n",
        "\n",
        "        # to apply RoPE Embeddings to --> q and k\n",
        "        self.rope = RotaryPositionalEmbedding(\n",
        "            theta = 10000.0,\n",
        "            d_k = self.head_size,\n",
        "            max_seq_len = seq_len\n",
        "        )\n",
        "\n",
        "        token_positions = torch.arange(seq_len, device=device)\n",
        "\n",
        "        query = self.rope(q, token_positions)\n",
        "        # print(f\"query shape after RoPE = {query.shape}\")\n",
        "        keys = self.rope(k, token_positions)\n",
        "        # print(f\"keys shape after RoPE = {keys.shape}\")\n",
        "        values = v\n",
        "\n",
        "        return self.scaled_dot_product_attention(query, keys, values, attn_mask=None, scale=None, is_causal=True).to(device)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query: torch.Tensor, keys: torch.Tensor, values: torch.Tensor,\n",
        "                        attn_mask: torch.Tensor, scale = float, is_causal = bool) -> torch.Tensor:\n",
        "        # \"\"\"\n",
        "        # Given key (K), query (Q), and value (V) tensors, return\n",
        "        # the output of your scaled dot product attention implementation.\n",
        "\n",
        "        # Args:\n",
        "        #     Q (Float[Tensor, \" ... queries d_k\"]): Query tensor\n",
        "        #     K (Float[Tensor, \" ... keys d_k\"]): Key tensor\n",
        "        #     V (Float[Tensor, \" ... values d_v\"]): Values tensor\n",
        "        #     mask (Float[Tensor, \" ... queries keys\"] | None): Mask tensor\n",
        "        # Returns:\n",
        "        #     Float[Tensor, \" ... queries d_v\"]: Output of SDPA\n",
        "        # \"\"\"\n",
        "        # print(f\"query shape = {query.shape}\")\n",
        "        # print(f\"keys shape = {keys.shape}\")\n",
        "        # print(f\"values shape = {values.shape}\")\n",
        "        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "        L = query.size(-2)\n",
        "        S = keys.size(-2)\n",
        "        attn_bias = torch.zeros(L, S, dtype = query.dtype).to(device)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask.to(device)\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_bias.masked_fill(attn_mask.logical_not(), float(\"-inf\"))\n",
        "            else:\n",
        "                attn_bias += attn_mask\n",
        "\n",
        "        if is_causal:\n",
        "            assert attn_mask is None\n",
        "            temp_mask = torch.ones(L, S, dtype = torch.bool).tril(diagonal=0).to(device)\n",
        "            attn_bias.masked_fill(temp_mask.logical_not(), float(\"-inf\"))\n",
        "            attn_bias.to(query.dtype)\n",
        "\n",
        "        attn_wei = ((query @ keys.transpose(-2, -1)) * scale_factor).to(device)\n",
        "        # print(f\"attn_wei shape = {attn_wei.shape}\")\n",
        "        # print(f\"attn_bias shape = {attn_bias.shape}\")\n",
        "        attn_wei += attn_bias.to(attn_wei.device)\n",
        "        # print(f\"attn_wei after adding bias shape = {attn_wei.shape}\")\n",
        "        softmax_attn_wei = Softmax(dim=-1, input=attn_wei).to(attn_wei.device)\n",
        "        # print(f\"softmax_attn_wei shape = {softmax_attn_wei.shape}\")\n",
        "        res = softmax_attn_wei @ values\n",
        "        # print(f\"res shape = {res.shape}\")\n",
        "        return res.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5263049d",
      "metadata": {
        "id": "5263049d"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = d_model // num_heads # --> d_k or d_v\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(d_model, self.head_size) for _ in range(num_heads)])\n",
        "        self.wo = Linear(self.d_model, self.d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # print(f\"Concatenated heads shape = {out.shape}\")\n",
        "        res = self.wo(out)\n",
        "        # print(f\"MultiheadAttention output shape = {res.shape}\")\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70fcffd",
      "metadata": {
        "id": "b70fcffd"
      },
      "outputs": [],
      "source": [
        "class transformer_block(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiheadAttention(d_model, num_heads)\n",
        "        self.ffn = SwiGLU_FeedForward(d_model)\n",
        "        self.rmsnorm = RMSNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        y = x + self.mha(self.rmsnorm(x))\n",
        "        # print(f\"y shape after MHA = {y.shape}\")\n",
        "        y = y + self.ffn(self.rmsnorm(y))\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ad3b7b",
      "metadata": {
        "id": "c6ad3b7b"
      },
      "outputs": [],
      "source": [
        "class transformer_lm(nn.Module):\n",
        "\n",
        "    def __init__(self, context_length: int, d_model: int, num_layers: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.context_length = context_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.token_embedding_table = Embedding(vocab_size, d_model)\n",
        "        self.position_embedding_table = Embedding(context_length, d_model)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[transformer_block(d_model, num_heads) for _ in range(num_layers)])\n",
        "        self.ln_f = RMSNorm(d_model, eps=1e-5)\n",
        "        self.lm_head = Linear(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, predicts: torch.Tensor):\n",
        "\n",
        "        B, T = predicts.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(predicts)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "\n",
        "        x = token_emb + pos_emb #(B, T, d_model)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) #(B, T, vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, index_vectors, max_tokens):\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            index_vectors = index_vectors[:, -self.context_length:]\n",
        "            logits = self(index_vectors)\n",
        "            logits = logits[:, -1, :]  # Focus on the last time step\n",
        "            probs = Softmax(dim=-1, input=logits)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)  # Sample the next token\n",
        "            index_vectors = torch.cat((index_vectors, next_token), dim=1)\n",
        "\n",
        "        return index_vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973ce4f0",
      "metadata": {
        "id": "973ce4f0"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(logits: torch.Tensor, targets: torch.Tensor):\n",
        "\n",
        "    # logits --> (batch_size, num_classes)\n",
        "    # targets -> (batch_size,)\n",
        "    batch_size, _ = logits.shape\n",
        "    # 1. We are doing this step to stabilize to not tend to inf\n",
        "    max_values, _ = torch.max(logits, dim=-1, keepdim=True)\n",
        "    stabilized_values = logits - max_values\n",
        "\n",
        "    # 2. Now we will calculate log-softmax\n",
        "    stabilized_exp = torch.exp(stabilized_values)\n",
        "    stabilized_exp_sum = torch.sum(stabilized_exp, dim=-1)\n",
        "\n",
        "    #. taking log\n",
        "    log_stabilized = torch.log(stabilized_exp_sum) + max_values.squeeze(-1)\n",
        "\n",
        "    # 3. Extract values for true classes\n",
        "    row_indices = torch.arange(batch_size) #batch_size\n",
        "    true_logits = logits[row_indices, targets]\n",
        "\n",
        "    loss_per_sample = log_stabilized - true_logits\n",
        "\n",
        "    loss = torch.mean(loss_per_sample)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60599948",
      "metadata": {
        "id": "60599948"
      },
      "outputs": [],
      "source": [
        "class AdamW(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the AdamW algorithm (Decoupled Weight Decay Regularization).\n",
        "\n",
        "    Parameters:\n",
        "        params (iterable): An iterable of torch.Tensor or dicts.\n",
        "        lr (float): Learning rate (default: 1e-3).\n",
        "        betas (Tuple[float, float]): Coefficients for running averages (default: (0.9, 0.999)).\n",
        "        eps (float): Term added to denominator for numerical stability (default: 1e-8).\n",
        "        weight_decay (float): Weight decay (L2 penalty) coefficient (default: 1e-2).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[torch.nn.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-8,\n",
        "        weight_decay: float = 1e-2\n",
        "    ):\n",
        "        # Validate inputs\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Learning rate should be >= 0: {lr}\")\n",
        "        if not (0.0 <= betas[0] < 1.0):\n",
        "            raise ValueError(f\"beta1 should be in [0, 1): {betas[0]}\")\n",
        "        if not (0.0 <= betas[1] < 1.0):\n",
        "            raise ValueError(f\"beta2 should be in [0, 1): {betas[1]}\")\n",
        "        if eps < 0.0:\n",
        "            raise ValueError(f\"Epsilon should be > 0: {eps}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Weight decay should be >= 0: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state: Any) -> None:\n",
        "        \"\"\"Support for pickling.\"\"\"\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "        # Ensure all parameter groups have the 'weight_decay' key in defaults\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('weight_decay', 1e-2)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure: Optional[callable] = None) -> Optional[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "\n",
        "        Returns:\n",
        "            Optional loss if closure is provided.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            weight_decay = group['weight_decay']\n",
        "            eps = group['eps']\n",
        "            lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
        "\n",
        "                # Get or initialize state\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                state['step'] += 1\n",
        "                step = state['step']\n",
        "\n",
        "                exp_avg = state['exp_avg']\n",
        "                exp_avg_sq = state['exp_avg_sq']\n",
        "\n",
        "                # Decay the first and second moment running averages\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # Inside step()\n",
        "                bias_correction1 = 1 - beta1 ** step\n",
        "                bias_correction2 = 1 - beta2 ** step\n",
        "\n",
        "                # Bias-corrected moments\n",
        "                hat_exp_avg = exp_avg / bias_correction1\n",
        "                hat_exp_avg_sq = exp_avg_sq / bias_correction2\n",
        "\n",
        "                # Denominator: sqrt(v_hat) + eps\n",
        "                denom = hat_exp_avg_sq.sqrt().add_(eps)\n",
        "\n",
        "                # Apply weight decay (AdamW: decoupled)\n",
        "                if weight_decay != 0:\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Update: θ ← θ - η * m_hat / sqrt(v_hat)\n",
        "                p.addcdiv_(hat_exp_avg, denom, value=-lr)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b5fbcd",
      "metadata": {
        "id": "44b5fbcd"
      },
      "outputs": [],
      "source": [
        "def learning_rate_scheduling(current_step: int, alpha_min: float, alpha_max: float, Tw: int, Tc: int):\n",
        "\n",
        "    t = current_step\n",
        "    final_alpha = 0.0\n",
        "\n",
        "    if t < Tw:\n",
        "        final_alpha = (t / Tw) * alpha_max\n",
        "    elif t >= Tw and t <= Tc:\n",
        "        x = (t - Tw) / (Tc - Tw)\n",
        "        final_alpha = alpha_min + (0.5 * (1 + math.cos(x * math.pi)) * (alpha_max - alpha_min))\n",
        "    else:\n",
        "        final_alpha = alpha_min\n",
        "\n",
        "    return final_alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b22752",
      "metadata": {
        "id": "84b22752"
      },
      "outputs": [],
      "source": [
        "def gradient_clipping(parameters, M):\n",
        "\n",
        "    l2_norm = 0.0\n",
        "\n",
        "    for p in parameters:\n",
        "        if p.grad is not None:\n",
        "            p_grad = p.grad\n",
        "            l2_norm += (p_grad ** 2).sum()\n",
        "\n",
        "    l2_norm = l2_norm.sqrt().item()\n",
        "    fact = M / (l2_norm + 1e-6)\n",
        "\n",
        "    if l2_norm > M:\n",
        "        for p in parameters:\n",
        "            if p.grad is not None:\n",
        "                p.grad.data.mul_(fact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495a76f8",
      "metadata": {
        "id": "495a76f8"
      },
      "outputs": [],
      "source": [
        "# def data_loading(x: torch.Tensor, batch_size: int, context_len: int, device: torch.device):\n",
        "\n",
        "#     total_tokens_needed = batch_size * context_len + 1\n",
        "\n",
        "#     x_trimmed = x[:total_tokens_needed]\n",
        "\n",
        "#     input_seq = x_trimmed[:-1].view(batch_size, context_len)\n",
        "#     target_seq = x_trimmed[1:].view(batch_size, context_len)\n",
        "\n",
        "#     return input_seq.to(device), target_seq.to(device)\n",
        "\n",
        "\n",
        "class MemMapDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, file_name: str, context_length: int):\n",
        "\n",
        "        self.data = np.memmap(file_name, dtype=np.int32, mode='r')\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.data) - self.context_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x_trimmed = self.data[idx : idx + self.context_length + 1]\n",
        "\n",
        "        input_seq = torch.from_numpy(x_trimmed[:-1]).long()\n",
        "        target_seq = torch.from_numpy(x_trimmed[1:]).long()\n",
        "\n",
        "        return input_seq, target_seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26223751",
      "metadata": {
        "id": "26223751"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, loss: float, step: int, filepath: str):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'step': step,\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved at step {step} with loss {loss:.4f} to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8f6b95",
      "metadata": {
        "id": "1b8f6b95"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, filepath: str):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    step = checkpoint['step']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Checkpoint loaded from {filepath} at step {step} with loss {loss:.4f}\")\n",
        "    return step, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6ba6e0",
      "metadata": {
        "id": "9e6ba6e0"
      },
      "outputs": [],
      "source": [
        "with open(\"tinystories_sample.txt\", 'r', encoding='utf-8') as f:\n",
        "    text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94dcfda5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "94dcfda5",
        "outputId": "8c9a3b15-4f14-4e3a-fd8e-f2a42360fbfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOnce upon a time there was a little boy named Ben. Ben loved to explore the world around him. He sa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "text_data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e7c318",
      "metadata": {
        "id": "15e7c318"
      },
      "outputs": [],
      "source": [
        "tokens = encode(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89fe905",
      "metadata": {
        "id": "f89fe905"
      },
      "outputs": [],
      "source": [
        "len_tokens = len(tokens)\n",
        "# tokens[:100]\n",
        "train_tokens_len = int(0.9 * len_tokens)\n",
        "train_tokens = tokens[:train_tokens_len]\n",
        "val_tokens = tokens[train_tokens_len:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# val_tokens\n",
        "len(train_tokens), len(val_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q73XRSW3WuNd",
        "outputId": "3e0ac163-d8e6-4fba-a2f3-e4bc0e43f362"
      },
      "id": "q73XRSW3WuNd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3240, 361)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70824f71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70824f71",
        "outputId": "61262efc-a30e-4d6a-a83b-c53f7f7e7c7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 10,  79, 110,  99, 256, 117, 112, 111, 110,  32], dtype=int32),\n",
              " array([ 98, 111, 117, 116,  32, 116, 104, 256, 115, 112], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "train_data = np.array(train_tokens, dtype=np.int32)\n",
        "val_data = np.array(val_tokens, dtype=np.int32)\n",
        "train_data[:10], val_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523f097e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523f097e",
        "outputId": "3299966c-8494-4f3d-a66d-27d36f5b3e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dat is written on train.bin file successfully\n"
          ]
        }
      ],
      "source": [
        "train_data.tofile(\"train.bin\")\n",
        "print(\"train_dat is written on train.bin file successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6179cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6179cd",
        "outputId": "58bddced-7edb-48c2-9ee3-27c87a1ddf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_data is written on val.bin file successfully\n"
          ]
        }
      ],
      "source": [
        "val_data.tofile(\"val.bin\")\n",
        "print(\"val_data is written on val.bin file successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94bb4af1",
      "metadata": {
        "id": "94bb4af1"
      },
      "outputs": [],
      "source": [
        "val_dataset = MemMapDataSet(\"val.bin\", context_length=context_length)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers = 2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e402a58",
      "metadata": {
        "id": "9e402a58"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model: torch.nn.Module, dataloader: DataLoader, device: torch.device):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    counts = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            xb, yb = x.to(device), y.to(device)\n",
        "            logits = model(xb)\n",
        "            B, T, C = logits.shape\n",
        "            loss = cross_entropy(logits.view(B * T, C), yb.view(B * T))\n",
        "            total_loss += loss.item()\n",
        "            counts += 1\n",
        "\n",
        "            if counts >= 1000:\n",
        "              break\n",
        "\n",
        "    # print(f\"Total evaluation batches: {total_batches}\")\n",
        "    avg_loss = total_loss / counts if counts > 0 else float('inf')\n",
        "    model.train()\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028936ce",
      "metadata": {
        "id": "028936ce"
      },
      "outputs": [],
      "source": [
        "# data = np.memmap('train.bin', dtype=np.int32, mode=\"r\")\n",
        "# idx = torch.from_numpy(data[:100])  # input + target\n",
        "# x = idx[:-1]  # input\n",
        "# y = idx[1:]  # target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a69b8f0",
      "metadata": {
        "id": "9a69b8f0"
      },
      "outputs": [],
      "source": [
        "# print(\"x:\", x)\n",
        "# print(\"y:\", y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816aefb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "816aefb6",
        "outputId": "0ebf7fd7-615f-4df2-a2a7-17113be02044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.509504 M parameters\n"
          ]
        }
      ],
      "source": [
        "lm = transformer_lm(context_length, d_model, n_layers)\n",
        "\n",
        "print(sum(p.numel() for p in lm.parameters()) / 1e6, \"M parameters\")\n",
        "\n",
        "optimizer = AdamW(lm.parameters(), lr=max_lr, betas=(0.9, 0.95), eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "train_dataset = MemMapDataSet(\"train.bin\", context_length)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "\n",
        "# for x, y in train_data_loader:\n",
        "#     print(f\"x shape = {x.shape}, y shape = {y.shape}\")\n",
        "#     print(x)\n",
        "#     print(y)\n",
        "#     break\n",
        "\n",
        "lm.train()\n",
        "steps = 0\n",
        "\n",
        "for x, y in train_data_loader:\n",
        "\n",
        "    if steps >= max_steps:\n",
        "        break\n",
        "\n",
        "    steps += 1\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    logits = lm(x)\n",
        "    loss = cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient clipping and learning rate scheduling\n",
        "    gradient_clipping(lm.parameters(), grad_clip)\n",
        "\n",
        "    lr = learning_rate_scheduling(steps, min_lr, max_lr, warmup_steps, total_cycle_steps)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 1000 == 0:\n",
        "        print(f\"Step {steps}, Loss: {loss.item():.4f}\")\n",
        "        save_checkpoint(lm, optimizer, loss.item(), steps, \"model_checkpoint.txt\")\n",
        "        val_loss = evaluate_model(lm, val_dataloader, device)\n",
        "        print(f\"Validation Loss after step {steps}: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32aeda69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32aeda69",
        "outputId": "9f7d6ce1-af62-42c6-f557-7a0cd67c1c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reom that day on, e e wbcwrrame 's onece bunnnenoeeencp. neennenzrrruennnnnneenneeunnn\n"
          ]
        }
      ],
      "source": [
        "story = lm.generate(index_vectors=torch.tensor([encode(\"Once upon a time,\")], device=device), max_tokens=1000)\n",
        "print(decode(story[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b15dab",
      "metadata": {
        "id": "b4b15dab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}